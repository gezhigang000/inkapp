# 模型适配层技术方案

## 目标

将当前硬编码的 Claude CLI 调用抽象为统一适配层，支持按配置切换不同大模型后端（Claude、DeepSeek、GPT-4o），同时将「联网搜索」能力从模型中解耦。

## 架构设计

### 核心思路：搜索与生成分离

```
Claude 模式（默认，一体化）：
  prompt ──→ [Claude CLI: 搜索 + 生成] ──→ HTML

非 Claude 模式（分离）：
  queries ──→ [search_adapter: Tavily/SerpAPI] ──→ 搜索结果
                                                      ↓
  prompt + 搜索结果 ──→ [llm_adapter: DeepSeek/OpenAI] ──→ HTML
```

### 文件结构

```
scripts/
├── llm_adapter.py          # LLM 调用适配层（新建）
├── search_adapter.py       # 搜索引擎适配层（新建）
├── daily_ai_news.py        # 主程序（已改用适配层）
├── video_analyzer.py       # 视频分析（已改用适配层）
└── ...
```

## 已实现内容

### llm_adapter.py

统一生成接口，3 个后端：

```python
def generate(prompt, config, timeout=600, need_search=True) -> str:
    """统一 LLM 生成入口"""

class LLMError(Exception):
    """LLM 调用异常的统一异常类"""
```

| 后端 | 实现函数 | 说明 |
|------|---------|------|
| Claude | `_generate_via_claude()` | 调用 claude CLI，`need_search=True` 时带 `--allowedTools WebSearch,WebFetch` |
| DeepSeek | `_generate_via_deepseek()` | 调用 `api.deepseek.com/v1/chat/completions` |
| OpenAI | `_generate_via_openai()` | 调用 `api.openai.com/v1/chat/completions` |

统一错误处理：超时、API 错误、空输出均抛 `LLMError`，由调用方决定退出策略。

### search_adapter.py

统一搜索接口，2 个后端：

```python
def search_and_fetch(queries, config, fetch_top_n=2) -> str:
    """执行多个搜索查询，返回格式化的上下文文本"""

def format_search_context(results) -> str:
    """将搜索结果格式化为 prompt 可用的文本块"""
```

| 后端 | 实现函数 | 说明 |
|------|---------|------|
| Tavily | `_search_via_tavily()` | Tavily API，自带正文提取，推荐首选 |
| SerpAPI | `_search_via_serpapi()` | SerpAPI + requests 抓取正文 |

仅当 `LLM_PROVIDER != claude` 时需要调用搜索适配器。

### daily_ai_news.py 改造点（3 处）

1. `generate_article(topic)` → `generate_article(topic, config)`，`main()` 中传入 config
2. `_generate_topic_research(topic, today)` → `_generate_topic_research(topic, today, config)`
   - Claude 模式：`generate(prompt, config, timeout=1200, need_search=True)`
   - 非 Claude：先 `search_and_fetch()` 获取上下文，注入 prompt 后调用 `generate()`
3. `_generate_daily_news(today)` → `_generate_daily_news(today, config)`
   - 同上模式，搜索 query 从公司名和话题构造

错误处理：`subprocess.TimeoutExpired` / `FileNotFoundError` 统一改为 catch `LLMError`。

### video_analyzer.py 改造点（1 处）

`call_claude_for_analysis(prompt)` → `call_llm_for_analysis(prompt, config)`

- Claude 后端：保留 `need_search=True`（模型可搜索补充背景）
- 非 Claude 后端：`need_search=False`（转录文本信息量足够）
- `analyze_video()` 已有 config 参数，直接透传

### config.env 新增配置项

```ini
# 大模型配置
LLM_PROVIDER=claude          # claude (默认) / deepseek / openai
DEEPSEEK_API_KEY=
DEEPSEEK_MODEL=deepseek-chat
OPENAI_MODEL=gpt-4o

# 搜索配置（仅 LLM_PROVIDER 非 claude 时生效）
SEARCH_PROVIDER=tavily       # tavily (推荐) / serpapi
TAVILY_API_KEY=
SERPAPI_API_KEY=
```

## 验证方法

```bash
# 1. 模块 import 测试
cd scripts && python3 -c "from llm_adapter import generate, LLMError; from search_adapter import search_and_fetch; print('OK')"

# 2. 默认配置（LLM_PROVIDER=claude）行为完全不变
cd /Users/gezhigang/weixin && ./run.sh --local --topic "测试"

# 3. 切换 DeepSeek（需配置 API Key）
# 在 config.env 中设置 LLM_PROVIDER=deepseek 和 DEEPSEEK_API_KEY
./run.sh --local --topic "测试"
```

## 注意事项

1. **默认行为不变**：`LLM_PROVIDER=claude` 时所有代码路径与改造前完全一致
2. **分离模式搜索质量**：Claude 一体化时模型自主决定搜索什么、读哪些页面。分离后搜索 query 需预先构造，可能不如模型自主搜索精准
3. **prompt 长度膨胀**：搜索结果注入 prompt 会增加 token 数，`_fetch_page_content()` 已做 3000 字符截断
4. **格式兜底**：非 Claude 模型可能需要额外 HTML 后处理逻辑（提取 `<section>` 块、补全内联样式）
